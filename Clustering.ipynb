{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/iranian_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1122936 entries, 0 to 1122935\n",
      "Data columns (total 31 columns):\n",
      "tweetid                     1122936 non-null int64\n",
      "userid                      1122936 non-null object\n",
      "user_display_name           1122936 non-null object\n",
      "user_screen_name            1122936 non-null object\n",
      "user_reported_location      887669 non-null object\n",
      "user_profile_description    995845 non-null object\n",
      "user_profile_url            434954 non-null object\n",
      "follower_count              1122936 non-null int64\n",
      "following_count             1122936 non-null int64\n",
      "account_creation_date       1122936 non-null object\n",
      "account_language            1122936 non-null object\n",
      "tweet_language              1117307 non-null object\n",
      "tweet_text                  1122936 non-null object\n",
      "tweet_time                  1122936 non-null object\n",
      "tweet_client_name           1100078 non-null object\n",
      "in_reply_to_tweetid         339350 non-null float64\n",
      "in_reply_to_userid          440244 non-null object\n",
      "quoted_tweet_tweetid        23369 non-null float64\n",
      "is_retweet                  1122936 non-null bool\n",
      "retweet_userid              232337 non-null object\n",
      "retweet_tweetid             232337 non-null float64\n",
      "latitude                    32 non-null float64\n",
      "longitude                   32 non-null float64\n",
      "quote_count                 1121572 non-null float64\n",
      "reply_count                 1121572 non-null float64\n",
      "like_count                  1121572 non-null float64\n",
      "retweet_count               1121572 non-null float64\n",
      "hashtags                    885731 non-null object\n",
      "urls                        1115553 non-null object\n",
      "user_mentions               685556 non-null object\n",
      "poll_choices                387 non-null object\n",
      "dtypes: bool(1), float64(9), int64(3), object(18)\n",
      "memory usage: 258.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[['userid','tweet_language','tweet_text','is_retweet','hashtags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.is_retweet==False)&(df.tweet_language=='en')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df.tweet_text.apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['@', 'ParkerLampe', 'An', 'inquiry', 'by', 'congress', 'confirms', 'that', 'ISIS', 'is', 'indeed', 'a', 'CIA', 'creation', 'http', ':', '//t.co/eFRmFwYZTV']),\n",
       "       list(['@', 'hadeelhmaidi', '@', 'wordpressdotcom', 'CIA', 'predict', 'third', 'terrorist', 'attack', 'after', 'Sidney', 'and', 'Pakiistan', 'in', 'USA', 'in', '3', 'days', 'http', ':', '//t.co/IrPx7M223N']),\n",
       "       list(['@', 'irfhabib', 'why', 'boko', 'haram', 'come', 'europe', ',', 'legally', 'and', 'easily', '?', 'http', ':', '//t.co/on2vzPqEPH'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tokens.values[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_tokenize(tweet):\n",
    "    twt = TweetTokenizer(reduce_len=True, strip_handles=True)\n",
    "    return twt.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_tokens'] = df.tweet_text.apply(lambda x: tweet_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['What', 'would', 'happen', 'to', 'you', 'if', 'you', 'were', 'a', 'political', 'dissent', 'in', 'Saudi', 'Arabia', '?', 'http://t.co/n1pHsz1UsX']),\n",
       "       list(['Saudi', 'embassy', 'in', 'Turkey', 'became', 'a', 'safe', 'haven', 'for', 'ISIL', 'terrorists', 'http://t.co/BsMnfC1xqU']),\n",
       "       list(['We', 'can', 'destroy', 'Israel', 'in', '‘', 'less', 'than', '12', 'minutes', '’', ':', 'Pakistani', 'commander', 'https://t.co/CSkZ4Z6ke1']),\n",
       "       list(['what', 'they', 'will', 'never', 'tell', 'you', 'about', 'Christmas', 'http://t.co/ovd4gGlWLE']),\n",
       "       list(['is', 'there', 'a', 'secret', 'collaboration', 'between', 'U', '.', 'S', 'and', 'Iran', 'against', 'isis', '?', 'http://t.co/Fw1IH3qOCb'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tweet_tokens.values[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_stops(tweet):\n",
    "    stops = set(nltk.corpus.stopwords.words('english'))\n",
    "    return [i.lower() for i in tweet if i.lower() not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "df['topic_words'] = df.tweet_tokens.apply(strip_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove links\n",
    "df['topic_words'] = df['topic_words'].apply(lambda x: [i for i in x if i[0:4]!='http'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stray punctuation\n",
    "import string\n",
    "punc = set(string.punctuation+'‘’…°–—“”')\n",
    "df['topic_words'] = df['topic_words'].apply(lambda x: [i for i in x if i[0] not in punc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['inquiry', 'congress', 'confirms', 'isis', 'indeed', 'cia', 'creation']),\n",
       "       list(['cia', 'predict', 'third', 'terrorist', 'attack', 'sidney', 'pakiistan', 'usa', '3', 'days']),\n",
       "       list(['boko', 'haram', 'come', 'europe', 'legally', 'easily']),\n",
       "       list(['isis', 'militants', 'plan', 'target', 'western', 'capitals']),\n",
       "       list(['turkish', 'intelligence', 'chief', 'isis', 'reality', 'optimistic', 'future'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.topic_words.values[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(tokens):\n",
    "    lem = nltk.stem.WordNetLemmatizer()\n",
    "    return [lem.lemmatize(i) for i in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemma'] = df.topic_words.apply(lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['inquiry', 'congress', 'confirms', 'isi', 'indeed', 'cia', 'creation']),\n",
       "       list(['cia', 'predict', 'third', 'terrorist', 'attack', 'sidney', 'pakiistan', 'usa', '3', 'day']),\n",
       "       list(['boko', 'haram', 'come', 'europe', 'legally', 'easily']),\n",
       "       list(['isi', 'militant', 'plan', 'target', 'western', 'capital']),\n",
       "       list(['turkish', 'intelligence', 'chief', 'isi', 'reality', 'optimistic', 'future']),\n",
       "       list(['would', 'happen', 'political', 'dissent', 'saudi', 'arabia']),\n",
       "       list(['saudi', 'embassy', 'turkey', 'became', 'safe', 'isil', 'terrorist']),\n",
       "       list(['destroy', 'israel', 'le', '12', 'minute', 'pakistani', 'commander']),\n",
       "       list(['never', 'tell', 'christmas']),\n",
       "       list(['secret', 'collaboration', 'u', 'iran', 'isi'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.lemma.values[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dum_preprocess(s):\n",
    "    return s\n",
    "def dum_tokenizer(s):\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats = 1000\n",
    "vctr = TfidfVectorizer(analyzer='word',tokenizer=dum_tokenizer,preprocessor=dum_preprocess,max_features=num_feats,token_pattern=None)\n",
    "X = vctr.fit_transform(df.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-6dab6e2b44c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtfidf_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    945\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tfidf_vals = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(vctr.get_feature_names())\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_results = pd.DataFrame(data = tfidf_vals, columns=feature_names)\n",
    "tfidf_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 5\n",
    "nmf = NMF(n_components=n_components, random_state=1, alpha=0.1, solver='mu', max_iter=1000, l1_ratio=0.5).fit(X)\n",
    "nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print(\"Topic #{0}: {1}\".format(topic_idx, top_words))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 10\n",
    "print_top_words(nmf, feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
